{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2980d72d-32a8-4a3c-a140-c2b4f6afdc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import script: No module named 'seaborn'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniTransformer(\n",
       "  (token_emb): Embedding(121, 128)\n",
       "  (pos_emb): Embedding(64, 128)\n",
       "  (blocks): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=121, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import MiniTransformer  # your model definition\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    import CLPSO_GRAD_script\n",
    "except Exception as e:\n",
    "    print(\"Failed to import script:\", e)\n",
    "\n",
    "checkpoint = torch.load(\"mini_llm_checkpoint.pt\", map_location='cpu')\n",
    "\n",
    "stoi = checkpoint['stoi']\n",
    "itos = checkpoint['itos']\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "\n",
    "# Recreate the model with same architecture\n",
    "model = MiniTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a04cf5-97a6-40c6-ace9-0c8c6393d093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead2f87e-f048-4459-9e14-8afd30490195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200,000,075 characters of text.\n",
      "Vocabulary size: 121\n",
      "Data shape: torch.Size([200000075])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load a small portion of the file\n",
    "max_chars = 200_000_000  # Adjust depending on your RAM (1 million = ~1MB of text)\n",
    "\n",
    "text = \"\"\n",
    "with open(\"TinyStories-train.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    while len(text) < max_chars:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        text += line\n",
    "\n",
    "print(f\"Loaded {len(text):,} characters of text.\")\n",
    "\n",
    "# Step 2: Build character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Step 3: Convert to tensor efficiently\n",
    "ids = [stoi[c] for c in text if c in stoi]\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593f1c71-e270-4206-af78-0e217b723127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Set model input parameters\n",
    "block_size = 64  # context window length\n",
    "batch_size = 32  # number of sequences per batch\n",
    "\n",
    "# Batch sampling function\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265e8b49-1384-4f73-81c8-8371c3c06ac2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CLPSO_GRAD_script' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Call the CLPSO fine-tuning function\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model, losses, precisions \u001b[38;5;241m=\u001b[39m \u001b[43mCLPSO_GRAD_script\u001b[49m\u001b[38;5;241m.\u001b[39mrun_clpso(\n\u001b[1;32m      5\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini_llm_checkpoint.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,           \u001b[38;5;66;03m# path to your pre-trained model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     get_batch_fn\u001b[38;5;241m=\u001b[39mget_batch,          \u001b[38;5;66;03m# your batch sampling function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[1;32m      8\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,           \u001b[38;5;66;03m# should match your token count\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     fine_tune_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,              \u001b[38;5;66;03m# tweak as needed\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     num_particles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m                  \u001b[38;5;66;03m# tweak based on GPU memory\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLPSO_GRAD_script' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Call the CLPSO fine-tuning function\n",
    "model, losses, precisions = CLPSO_GRAD_script.run_clpso(\n",
    "    model_path=\"mini_llm_checkpoint.pt\",           # path to your pre-trained model\n",
    "    get_batch_fn=get_batch,          # your batch sampling function\n",
    "    criterion=criterion,\n",
    "    vocab_size=vocab_size,           # should match your token count\n",
    "    fine_tune_epochs=3,              # tweak as needed\n",
    "    num_particles=5                  # tweak based on GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed68467f-96a3-4712-a3fb-65d9f41945c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_text, max_new_tokens=20, block_size=64, temperature=0.8):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = input_ids[:, -block_size:]  # crop context window\n",
    "        logits = model(x_cond)               # [B, T, vocab]\n",
    "        probs = F.softmax(logits[:, -1, :]/ temperature, dim=-1)  # last token\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # sample\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    return ''.join([itos[i] for i in input_ids[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a13486b-6eeb-4048-945d-4f0e2be91c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a cat whoh ohckhh chhh crr hhhhhh hhhhakh hhh chhir that he shair the happy lot. The stom was it the stor sta\n"
     ]
    }
   ],
   "source": [
    "output = generate(model, \"Once there was a cat who\",max_new_tokens=100,temperature=0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742170e-e41c-4048-9d67-4b513935a790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
