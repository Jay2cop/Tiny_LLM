{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fed2258-dee4-4ddb-88d6-3371b3ad5d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /school/GPTEngineer_conda/Miniconda3/envs/LLM_test/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2980d72d-32a8-4a3c-a140-c2b4f6afdc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniTransformer(\n",
       "  (token_emb): Embedding(121, 128)\n",
       "  (pos_emb): Embedding(64, 128)\n",
       "  (blocks): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=121, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import MiniTransformer  # your model definition\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import CLPSO_GRAD_script\n",
    "except Exception as e:\n",
    "    print(\"Failed to import script:\", e)\n",
    "\n",
    "checkpoint = torch.load(\"mini_llm_checkpoint.pt\", map_location='cpu')\n",
    "\n",
    "stoi = checkpoint['stoi']\n",
    "itos = checkpoint['itos']\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "\n",
    "# Recreate the model with same architecture\n",
    "model = MiniTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a04cf5-97a6-40c6-ace9-0c8c6393d093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead2f87e-f048-4459-9e14-8afd30490195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200,000,075 characters of text.\n",
      "Vocabulary size: 121\n",
      "Data shape: torch.Size([200000075])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load a small portion of the file\n",
    "max_chars = 200_000_000  # Adjust depending on your RAM (1 million = ~1MB of text)\n",
    "\n",
    "text = \"\"\n",
    "with open(\"TinyStories-train.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    while len(text) < max_chars:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        text += line\n",
    "\n",
    "print(f\"Loaded {len(text):,} characters of text.\")\n",
    "\n",
    "# Step 2: Build character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Step 3: Convert to tensor efficiently\n",
    "ids = [stoi[c] for c in text if c in stoi]\n",
    "data = torch.tensor(ids, dtype=torch.long)\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593f1c71-e270-4206-af78-0e217b723127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Set model input parameters\n",
    "block_size = 64  # context window length\n",
    "batch_size = 32  # number of sequences per batch\n",
    "\n",
    "# Batch sampling function\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265e8b49-1384-4f73-81c8-8371c3c06ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Global Best Fitness (from pre-trained head): 0.8042\n",
      "Epoch 1/100\n",
      "New global best fitness: 0.7709\n",
      "New global best fitness: 0.7471\n",
      "New global best fitness: 0.7383\n",
      "New global best fitness: 0.7198\n",
      "Epoch 1/100 - Val Loss: 0.8196, Val Acc: 74.12%\n",
      "Epoch completed in 2.89s. Best Global Fitness: 0.7198\n",
      "Epoch 2/100\n",
      "Epoch 2/100 - Val Loss: 0.8121, Val Acc: 73.14%\n",
      "Epoch completed in 2.60s. Best Global Fitness: 0.7198\n",
      "Epoch 3/100\n",
      "Epoch 3/100 - Val Loss: 0.7594, Val Acc: 76.07%\n",
      "Epoch completed in 2.53s. Best Global Fitness: 0.7198\n",
      "Epoch 4/100\n",
      "Epoch 4/100 - Val Loss: 0.7844, Val Acc: 74.90%\n",
      "Epoch completed in 2.51s. Best Global Fitness: 0.7198\n",
      "Epoch 5/100\n",
      "Epoch 5/100 - Val Loss: 0.7940, Val Acc: 74.90%\n",
      "Epoch completed in 2.67s. Best Global Fitness: 0.7198\n",
      "Epoch 6/100\n",
      "Epoch 6/100 - Val Loss: 0.8063, Val Acc: 72.85%\n",
      "Epoch completed in 2.56s. Best Global Fitness: 0.7198\n",
      "Epoch 7/100\n",
      "Epoch 7/100 - Val Loss: 0.8140, Val Acc: 73.78%\n",
      "Epoch completed in 2.59s. Best Global Fitness: 0.7198\n",
      "Epoch 8/100\n",
      "Epoch 8/100 - Val Loss: 0.7629, Val Acc: 75.73%\n",
      "Epoch completed in 2.49s. Best Global Fitness: 0.7198\n",
      "Early stopping triggered.\n",
      "\n",
      "Optimization completed. Final Best Global Fitness: 0.7198\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Call the CLPSO fine-tuning function\n",
    "model, losses, precisions = CLPSO_GRAD_script.run_clpso(\n",
    "    model_path=\"mini_llm_checkpoint.pt\",           # path to your pre-trained model\n",
    "    get_batch_fn=get_batch,          # your batch sampling function\n",
    "    criterion=criterion,\n",
    "    vocab_size=vocab_size,           # should match your token count\n",
    "    fine_tune_epochs=100,              # tweak as needed\n",
    "    num_particles=30,                  # tweak based on GPU memory\n",
    "    w=0.5,\n",
    "    c1=1.8,\n",
    "    c2=1.2,\n",
    "    bounds=0.1,\n",
    "    p_threshold=0.05,\n",
    "    gd_learning_rate=0.006,\n",
    "    gd_weight_decay=0.008,\n",
    "    num_grad_steps=6,\n",
    "    num_eval_batches=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea81f823-5dda-480f-b3b0-759dac92691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'stoi': stoi,\n",
    "    'itos': itos,\n",
    "    'vocab_size': vocab_size\n",
    "}, \"finetuned_llm_clpso.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "384fdcd4-a9a8-486e-8e6c-3631775418f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniTransformer(\n",
       "  (token_emb): Embedding(121, 128)\n",
       "  (pos_emb): Embedding(64, 128)\n",
       "  (blocks): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=128, out_features=121, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the checkpoint\n",
    "checkpoint = torch.load(\"finetuned_llm_clpso.pt\", map_location='cpu')\n",
    "\n",
    "# Step 2: Restore vocabulary and mappings\n",
    "stoi = checkpoint['stoi']\n",
    "itos = checkpoint['itos']\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "\n",
    "# Step 3: Rebuild the model and load weights\n",
    "model = MiniTransformer(vocab_size=vocab_size)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval().to(device)  # don't forget to move to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed68467f-96a3-4712-a3fb-65d9f41945c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_text, max_new_tokens=200, block_size=64, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long)[None, :].to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = input_ids[:, -block_size:]\n",
    "        logits = model(x_cond)\n",
    "        logits = logits[:, -1, :] / temperature  # only the last token's logits\n",
    "\n",
    "        # Apply top_k filtering\n",
    "        if top_k is not None:\n",
    "            values, indices = torch.topk(logits, top_k)\n",
    "            probs = torch.zeros_like(logits).scatter_(1, indices, values)\n",
    "            probs = F.softmax(probs, dim=-1)\n",
    "        else:\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        if itos[next_token.item()] == '<|endoftext|>':\n",
    "            break\n",
    "\n",
    "    return ''.join([itos[i] for i in input_ids[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a13486b-6eeb-4048-945d-4f0e2be91c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a robot named Timmy. Timmy loved to play outside in the sun. One day, he saw a big ball and noticed that Max was falling on the ground and started to cry.\n"
     ]
    }
   ],
   "source": [
    "output = generate(model, \"Once upon a time there was a robot\",max_new_tokens=400,temperature=0.7,top_k=30)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e1e79-ce59-44c5-be24-dd84c7a7e198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
